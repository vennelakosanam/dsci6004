{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gp-crvAI1aXf",
    "outputId": "de846b04-ea87-4f0f-9f1c-0cf35c4f8d83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Summaries', 'News Articles']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "# Path to the ZIP file\n",
    "zip_path = 'Summaries.zip'\n",
    "extract_path = 'Summaries'\n",
    "\n",
    "# Extract the ZIP file\n",
    "with ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "# List the contents of the extracted directory\n",
    "extracted_files = os.listdir(extract_path)\n",
    "extracted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qb5oePW31nHV",
    "outputId": "5c015a60-4c5c-45cd-b987-dd8c047bd443"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['entertainment', 'business', 'tech', 'politics', 'sport'],\n",
       " ['entertainment', 'business', 'tech', 'politics', 'sport'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths to the inner directories\n",
    "news_articles_path = os.path.join(extract_path, 'News Articles')\n",
    "summaries_path = os.path.join(extract_path, 'Summaries')\n",
    "\n",
    "# List the contents of the News Articles and Summaries directories\n",
    "news_articles_files = os.listdir(news_articles_path)\n",
    "summaries_files = os.listdir(summaries_path)\n",
    "\n",
    "news_articles_files, summaries_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTa4OW4U1w-N",
    "outputId": "935a9c6e-e358-4a5c-8765-1b3f2f63d435"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['167.txt', '186.txt', '286.txt', '099.txt', '211.txt'],\n",
       " ['167.txt', '186.txt', '286.txt', '099.txt', '211.txt'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to one of the category directories for both articles and summaries\n",
    "category_news_path = os.path.join(news_articles_path, 'business')\n",
    "category_summaries_path = os.path.join(summaries_path, 'business')\n",
    "\n",
    "# List some of the files in these directories\n",
    "category_news_files = os.listdir(category_news_path)[:5]  # List first 5 files\n",
    "category_summaries_files = os.listdir(category_summaries_path)[\n",
    "    :5]  # List first 5 files\n",
    "\n",
    "# Display filenames\n",
    "category_news_files, category_summaries_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iU5pqCgT13K6",
    "outputId": "0c300930-3ed0-47e5-c8ed-4c4583dffd7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\\n\\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n\\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL\\'s underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL\\'s existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\\n\\nTime Warner\\'s fourth quarter profits were slightly better than analysts\\' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.\\n\\nTimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann\\'s purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\\n',\n",
       " \"TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn.For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn.Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues.Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters.Time Warner's fourth quarter profits were slightly better than analysts' expectations.\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the contents of the first news article and its summary\n",
    "with open(os.path.join(category_news_path, '001.txt'), 'r', encoding='utf-8') as file:\n",
    "    news_article_sample = file.read()\n",
    "\n",
    "with open(os.path.join(category_summaries_path, '001.txt'), 'r', encoding='utf-8') as file:\n",
    "    summary_sample = file.read()\n",
    "\n",
    "news_article_sample, summary_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kvz6l_3k17b6"
   },
   "outputs": [],
   "source": [
    "def load_data_with_fallback_encoding(category_path, summaries_path):\n",
    "    articles = []\n",
    "    summaries = []\n",
    "\n",
    "    # Get all article files in the category\n",
    "    article_files = sorted(os.listdir(category_path))\n",
    "\n",
    "    for article_file in article_files:\n",
    "        article_path = os.path.join(category_path, article_file)\n",
    "        summary_path = os.path.join(summaries_path, article_file)\n",
    "\n",
    "        # Try to read the article and summary files with a fallback encoding\n",
    "        try:\n",
    "            with open(article_path, 'r', encoding='utf-8') as file:\n",
    "                article = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(article_path, 'r', encoding='iso-8859-1') as file:\n",
    "                article = file.read()\n",
    "\n",
    "        try:\n",
    "            with open(summary_path, 'r', encoding='utf-8') as file:\n",
    "                summary = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(summary_path, 'r', encoding='iso-8859-1') as file:\n",
    "                summary = file.read()\n",
    "\n",
    "        articles.append(article)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return articles, summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "I4riVJLq18OJ"
   },
   "outputs": [],
   "source": [
    "# Reload data from each category with fallback encoding\n",
    "categories = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "all_articles = []\n",
    "all_summaries = []\n",
    "all_categories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0TqDquV1-4L",
    "outputId": "1d9c4f34-8b60-4c5b-f2ec-b86698fa1eb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Category                                            Article  \\\n",
       " 0  business  Ad sales boost Time Warner profit\\n\\nQuarterly...   \n",
       " 1  business  Dollar gains on Greenspan speech\\n\\nThe dollar...   \n",
       " 2  business  Yukos unit buyer faces loan claim\\n\\nThe owner...   \n",
       " 3  business  High fuel prices hit BA's profits\\n\\nBritish A...   \n",
       " 4  business  Pernod takeover talk lifts Domecq\\n\\nShares in...   \n",
       " \n",
       "                                              Summary  \n",
       " 0  TimeWarner said fourth quarter sales rose 2% t...  \n",
       " 1  The dollar has hit its highest level against t...  \n",
       " 2  Yukos' owner Menatep Group says it will ask Ro...  \n",
       " 3  Rod Eddington, BA's chief executive, said the ...  \n",
       " 4  Pernod has reduced the debt it took on to fund...  ,\n",
       " (2225, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "for category in categories:\n",
    "    category_news_path = os.path.join(news_articles_path, category)\n",
    "    category_summaries_path = os.path.join(summaries_path, category)\n",
    "    articles, summaries = load_data_with_fallback_encoding(\n",
    "        category_news_path, category_summaries_path)\n",
    "\n",
    "    all_articles.extend(articles)\n",
    "    all_summaries.extend(summaries)\n",
    "    all_categories.extend([category] * len(articles))\n",
    "\n",
    "# Create a DataFrame with the loaded data\n",
    "data_df = pd.DataFrame({\n",
    "    'Category': all_categories,\n",
    "    'Article': all_articles,\n",
    "    'Summary': all_summaries\n",
    "})\n",
    "\n",
    "data_df.head(), data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nEKfqC4i2FzZ",
    "outputId": "0404e881-c488-4bf5-bda7-731287c142fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1780, 3), (222, 3), (223, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and temporary sets (temporary set will be further split into validation and test)\n",
    "train_df, temp_df = train_test_split(data_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temporary set into validation and test sets\n",
    "validation_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_df.shape, validation_df.shape, test_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KHblc4LV2N2x",
    "outputId": "a875228b-11f8-4b3e-8e21-ee9001beeebc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "# Use a smaller model if possible, like BART-base\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "PqFaTM8Q2OjP"
   },
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, tokenizer, articles, summaries, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.articles = articles\n",
    "        self.summaries = summaries\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article_encoded = self.tokenizer(self.articles[idx], return_tensors='pt',\n",
    "                                         truncation=True, padding='max_length', max_length=self.max_length)\n",
    "        summary_encoded = self.tokenizer(self.summaries[idx], return_tensors='pt',\n",
    "                                         truncation=True, padding='max_length', max_length=self.max_length)\n",
    "        return {\n",
    "            'input_ids': article_encoded['input_ids'].squeeze(0),\n",
    "            'attention_mask': article_encoded['attention_mask'].squeeze(0),\n",
    "            'labels': summary_encoded['input_ids'].squeeze(0)\n",
    "        }\n",
    "\n",
    "train_dataset = SummarizationDataset(tokenizer, train_df['Article'].tolist(), train_df['Summary'].tolist())\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Reduced batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-c1dyoo2SBz",
    "outputId": "07113308-4551-49d0-ebe4-0cb29d961686"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 5.66681432723999\n",
      "Epoch: 0, Loss: 5.7609100341796875\n",
      "Epoch: 0, Loss: 6.137808799743652\n",
      "Epoch: 0, Loss: 4.6346611976623535\n",
      "Epoch: 0, Loss: 4.375171184539795\n",
      "Epoch: 0, Loss: 5.533237457275391\n",
      "Epoch: 0, Loss: 4.785006046295166\n",
      "Epoch: 0, Loss: 4.30952787399292\n",
      "Epoch: 0, Loss: 4.129088401794434\n",
      "Epoch: 0, Loss: 4.824732780456543\n",
      "Epoch: 0, Loss: 3.848839282989502\n",
      "Epoch: 0, Loss: 4.196347713470459\n",
      "Epoch: 0, Loss: 3.4949588775634766\n",
      "Epoch: 0, Loss: 4.281175136566162\n",
      "Epoch: 0, Loss: 4.053802490234375\n",
      "Epoch: 0, Loss: 4.114142417907715\n",
      "Epoch: 0, Loss: 3.7110671997070312\n",
      "Epoch: 0, Loss: 3.3556606769561768\n",
      "Epoch: 0, Loss: 3.377974033355713\n",
      "Epoch: 0, Loss: 3.281545639038086\n",
      "Epoch: 0, Loss: 3.3881051540374756\n",
      "Epoch: 0, Loss: 3.2720024585723877\n",
      "Epoch: 0, Loss: 2.9274868965148926\n",
      "Epoch: 0, Loss: 2.7139949798583984\n",
      "Epoch: 0, Loss: 3.047201633453369\n",
      "Epoch: 0, Loss: 2.741421699523926\n",
      "Epoch: 0, Loss: 3.129354953765869\n",
      "Epoch: 0, Loss: 2.1966893672943115\n",
      "Epoch: 0, Loss: 2.502671241760254\n",
      "Epoch: 0, Loss: 3.1634469032287598\n",
      "Epoch: 0, Loss: 2.840991497039795\n",
      "Epoch: 0, Loss: 2.5184805393218994\n",
      "Epoch: 0, Loss: 2.536968469619751\n",
      "Epoch: 0, Loss: 2.6339328289031982\n",
      "Epoch: 0, Loss: 2.7954840660095215\n",
      "Epoch: 0, Loss: 2.146794557571411\n",
      "Epoch: 0, Loss: 2.2259864807128906\n",
      "Epoch: 0, Loss: 2.243317127227783\n",
      "Epoch: 0, Loss: 2.483957290649414\n",
      "Epoch: 0, Loss: 2.174888849258423\n",
      "Epoch: 0, Loss: 1.9608511924743652\n",
      "Epoch: 0, Loss: 2.0004007816314697\n",
      "Epoch: 0, Loss: 1.9902273416519165\n",
      "Epoch: 0, Loss: 2.234618902206421\n",
      "Epoch: 0, Loss: 1.964850664138794\n",
      "Epoch: 0, Loss: 1.7298853397369385\n",
      "Epoch: 0, Loss: 1.8059923648834229\n",
      "Epoch: 0, Loss: 1.960125207901001\n",
      "Epoch: 0, Loss: 1.8038009405136108\n",
      "Epoch: 0, Loss: 1.8893089294433594\n",
      "Epoch: 0, Loss: 1.4883809089660645\n",
      "Epoch: 0, Loss: 1.6254980564117432\n",
      "Epoch: 0, Loss: 1.6960954666137695\n",
      "Epoch: 0, Loss: 1.722058892250061\n",
      "Epoch: 0, Loss: 2.014853000640869\n",
      "Epoch: 0, Loss: 1.6299729347229004\n",
      "Epoch: 0, Loss: 1.767920970916748\n",
      "Epoch: 0, Loss: 1.1691360473632812\n",
      "Epoch: 0, Loss: 1.8246276378631592\n",
      "Epoch: 0, Loss: 1.9814225435256958\n",
      "Epoch: 0, Loss: 1.8383322954177856\n",
      "Epoch: 0, Loss: 1.8962692022323608\n",
      "Epoch: 0, Loss: 1.6550110578536987\n",
      "Epoch: 0, Loss: 1.0273785591125488\n",
      "Epoch: 0, Loss: 1.950136661529541\n",
      "Epoch: 0, Loss: 2.0951309204101562\n",
      "Epoch: 0, Loss: 1.422279953956604\n",
      "Epoch: 0, Loss: 1.7335326671600342\n",
      "Epoch: 0, Loss: 1.801480770111084\n",
      "Epoch: 0, Loss: 1.2815614938735962\n",
      "Epoch: 0, Loss: 0.9976296424865723\n",
      "Epoch: 0, Loss: 1.2680253982543945\n",
      "Epoch: 0, Loss: 1.0441513061523438\n",
      "Epoch: 0, Loss: 1.2535223960876465\n",
      "Epoch: 0, Loss: 0.9599056243896484\n",
      "Epoch: 0, Loss: 1.2270292043685913\n",
      "Epoch: 0, Loss: 1.293065071105957\n",
      "Epoch: 0, Loss: 1.135166883468628\n",
      "Epoch: 0, Loss: 2.331530809402466\n",
      "Epoch: 0, Loss: 1.0573049783706665\n",
      "Epoch: 0, Loss: 1.5033161640167236\n",
      "Epoch: 0, Loss: 1.407357096672058\n",
      "Epoch: 0, Loss: 1.1253424882888794\n",
      "Epoch: 0, Loss: 1.3581167459487915\n",
      "Epoch: 0, Loss: 1.1314998865127563\n",
      "Epoch: 0, Loss: 1.351048469543457\n",
      "Epoch: 0, Loss: 1.335323452949524\n",
      "Epoch: 0, Loss: 1.4982666969299316\n",
      "Epoch: 0, Loss: 1.183933973312378\n",
      "Epoch: 0, Loss: 1.3672479391098022\n",
      "Epoch: 0, Loss: 1.4551942348480225\n",
      "Epoch: 0, Loss: 1.081961989402771\n",
      "Epoch: 0, Loss: 1.4760401248931885\n",
      "Epoch: 0, Loss: 0.9626666307449341\n",
      "Epoch: 0, Loss: 0.7365144491195679\n",
      "Epoch: 0, Loss: 1.6117208003997803\n",
      "Epoch: 0, Loss: 0.5846460461616516\n",
      "Epoch: 0, Loss: 1.3872582912445068\n",
      "Epoch: 0, Loss: 1.7773659229278564\n",
      "Epoch: 0, Loss: 1.3547459840774536\n",
      "Epoch: 0, Loss: 0.8853127360343933\n",
      "Epoch: 0, Loss: 0.9053017497062683\n",
      "Epoch: 0, Loss: 1.1472270488739014\n",
      "Epoch: 0, Loss: 0.9388208985328674\n",
      "Epoch: 0, Loss: 1.1409001350402832\n",
      "Epoch: 0, Loss: 0.5053384304046631\n",
      "Epoch: 0, Loss: 1.2194325923919678\n",
      "Epoch: 0, Loss: 0.9299713969230652\n",
      "Epoch: 0, Loss: 0.9173116683959961\n",
      "Epoch: 0, Loss: 0.8192512392997742\n",
      "Epoch: 0, Loss: 1.8611279726028442\n",
      "Epoch: 0, Loss: 1.5510568618774414\n",
      "Epoch: 0, Loss: 1.6505513191223145\n",
      "Epoch: 0, Loss: 1.7291045188903809\n",
      "Epoch: 0, Loss: 1.2680386304855347\n",
      "Epoch: 0, Loss: 1.037990927696228\n",
      "Epoch: 0, Loss: 1.1300007104873657\n",
      "Epoch: 0, Loss: 0.7560545206069946\n",
      "Epoch: 0, Loss: 1.1365857124328613\n",
      "Epoch: 0, Loss: 1.5865885019302368\n",
      "Epoch: 0, Loss: 0.8966790437698364\n",
      "Epoch: 0, Loss: 1.5110433101654053\n",
      "Epoch: 0, Loss: 0.8073779940605164\n",
      "Epoch: 0, Loss: 1.5948631763458252\n",
      "Epoch: 0, Loss: 1.331620216369629\n",
      "Epoch: 0, Loss: 1.4108233451843262\n",
      "Epoch: 0, Loss: 1.0646480321884155\n",
      "Epoch: 0, Loss: 1.1370245218276978\n",
      "Epoch: 0, Loss: 1.220002293586731\n",
      "Epoch: 0, Loss: 1.0275779962539673\n",
      "Epoch: 0, Loss: 0.9576376676559448\n",
      "Epoch: 0, Loss: 1.2366299629211426\n",
      "Epoch: 0, Loss: 1.1624345779418945\n",
      "Epoch: 0, Loss: 1.2352564334869385\n",
      "Epoch: 0, Loss: 1.3817144632339478\n",
      "Epoch: 0, Loss: 1.7958333492279053\n",
      "Epoch: 0, Loss: 0.5562370419502258\n",
      "Epoch: 0, Loss: 1.2915759086608887\n",
      "Epoch: 0, Loss: 1.5106682777404785\n",
      "Epoch: 0, Loss: 1.0224405527114868\n",
      "Epoch: 0, Loss: 1.5122615098953247\n",
      "Epoch: 0, Loss: 1.19050931930542\n",
      "Epoch: 0, Loss: 1.1944832801818848\n",
      "Epoch: 0, Loss: 1.2295225858688354\n",
      "Epoch: 0, Loss: 0.7347177863121033\n",
      "Epoch: 0, Loss: 1.2645045518875122\n",
      "Epoch: 0, Loss: 1.2204432487487793\n",
      "Epoch: 0, Loss: 1.4576672315597534\n",
      "Epoch: 0, Loss: 1.25961434841156\n",
      "Epoch: 0, Loss: 1.343705654144287\n",
      "Epoch: 0, Loss: 1.1082438230514526\n",
      "Epoch: 0, Loss: 1.123676061630249\n",
      "Epoch: 0, Loss: 0.6578267812728882\n",
      "Epoch: 0, Loss: 1.2128797769546509\n",
      "Epoch: 0, Loss: 1.0243330001831055\n",
      "Epoch: 0, Loss: 0.6872156858444214\n",
      "Epoch: 0, Loss: 1.5439777374267578\n",
      "Epoch: 0, Loss: 1.1422234773635864\n",
      "Epoch: 0, Loss: 2.1422524452209473\n",
      "Epoch: 0, Loss: 1.192102313041687\n",
      "Epoch: 0, Loss: 1.9043283462524414\n",
      "Epoch: 0, Loss: 1.9886995553970337\n",
      "Epoch: 0, Loss: 0.7194786071777344\n",
      "Epoch: 0, Loss: 1.084506630897522\n",
      "Epoch: 0, Loss: 1.1040868759155273\n",
      "Epoch: 0, Loss: 1.0633530616760254\n",
      "Epoch: 0, Loss: 0.6129156947135925\n",
      "Epoch: 0, Loss: 1.4567791223526\n",
      "Epoch: 0, Loss: 1.2831329107284546\n",
      "Epoch: 0, Loss: 2.0123846530914307\n",
      "Epoch: 0, Loss: 1.3801884651184082\n",
      "Epoch: 0, Loss: 1.6980869770050049\n",
      "Epoch: 0, Loss: 1.42251455783844\n",
      "Epoch: 0, Loss: 1.0397459268569946\n",
      "Epoch: 0, Loss: 1.7726051807403564\n",
      "Epoch: 0, Loss: 1.707677960395813\n",
      "Epoch: 0, Loss: 1.2619719505310059\n",
      "Epoch: 0, Loss: 1.2641069889068604\n",
      "Epoch: 0, Loss: 1.317073941230774\n",
      "Epoch: 0, Loss: 1.7579721212387085\n",
      "Epoch: 0, Loss: 0.7033509016036987\n",
      "Epoch: 0, Loss: 0.7674697041511536\n",
      "Epoch: 0, Loss: 1.093503475189209\n",
      "Epoch: 0, Loss: 0.9503402709960938\n",
      "Epoch: 0, Loss: 1.5318617820739746\n",
      "Epoch: 0, Loss: 1.2923520803451538\n",
      "Epoch: 0, Loss: 0.922281801700592\n",
      "Epoch: 0, Loss: 0.7414167523384094\n",
      "Epoch: 0, Loss: 0.9391177296638489\n",
      "Epoch: 0, Loss: 0.8003932237625122\n",
      "Epoch: 0, Loss: 1.4753535985946655\n",
      "Epoch: 0, Loss: 1.3184360265731812\n",
      "Epoch: 0, Loss: 0.8773217797279358\n",
      "Epoch: 0, Loss: 1.1747803688049316\n",
      "Epoch: 0, Loss: 1.4524263143539429\n",
      "Epoch: 0, Loss: 1.5107007026672363\n",
      "Epoch: 0, Loss: 1.0069587230682373\n",
      "Epoch: 0, Loss: 1.1113556623458862\n",
      "Epoch: 0, Loss: 1.0899696350097656\n",
      "Epoch: 0, Loss: 1.5605299472808838\n",
      "Epoch: 0, Loss: 1.434306025505066\n",
      "Epoch: 0, Loss: 0.7594302892684937\n",
      "Epoch: 0, Loss: 0.7815359234809875\n",
      "Epoch: 0, Loss: 1.5487346649169922\n",
      "Epoch: 0, Loss: 1.0709797143936157\n",
      "Epoch: 0, Loss: 1.0563457012176514\n",
      "Epoch: 0, Loss: 0.7191359996795654\n",
      "Epoch: 0, Loss: 1.9305835962295532\n",
      "Epoch: 0, Loss: 0.9841775894165039\n",
      "Epoch: 0, Loss: 1.107113242149353\n",
      "Epoch: 0, Loss: 1.5416315793991089\n",
      "Epoch: 0, Loss: 1.3000046014785767\n",
      "Epoch: 0, Loss: 1.6371605396270752\n",
      "Epoch: 0, Loss: 1.1731154918670654\n",
      "Epoch: 0, Loss: 0.6670485734939575\n",
      "Epoch: 0, Loss: 1.0431272983551025\n",
      "Epoch: 0, Loss: 0.8487659096717834\n",
      "Epoch: 0, Loss: 1.5154898166656494\n",
      "Epoch: 0, Loss: 1.2321276664733887\n",
      "Epoch: 0, Loss: 1.955812931060791\n",
      "Epoch: 0, Loss: 0.48664796352386475\n",
      "Epoch: 0, Loss: 1.4075233936309814\n",
      "Epoch: 0, Loss: 0.8851640224456787\n",
      "Epoch: 1, Loss: 1.4306988716125488\n",
      "Epoch: 1, Loss: 0.9692420363426208\n",
      "Epoch: 1, Loss: 1.1386326551437378\n",
      "Epoch: 1, Loss: 1.5732011795043945\n",
      "Epoch: 1, Loss: 1.077689290046692\n",
      "Epoch: 1, Loss: 1.1240609884262085\n",
      "Epoch: 1, Loss: 1.0191282033920288\n",
      "Epoch: 1, Loss: 0.719178318977356\n",
      "Epoch: 1, Loss: 0.8974616527557373\n",
      "Epoch: 1, Loss: 1.1825964450836182\n",
      "Epoch: 1, Loss: 1.20088791847229\n",
      "Epoch: 1, Loss: 2.0350210666656494\n",
      "Epoch: 1, Loss: 1.3971189260482788\n",
      "Epoch: 1, Loss: 1.0255905389785767\n",
      "Epoch: 1, Loss: 1.3240044116973877\n",
      "Epoch: 1, Loss: 0.9800292253494263\n",
      "Epoch: 1, Loss: 1.243679404258728\n",
      "Epoch: 1, Loss: 0.9717675447463989\n",
      "Epoch: 1, Loss: 1.146721363067627\n",
      "Epoch: 1, Loss: 0.872636079788208\n",
      "Epoch: 1, Loss: 1.3668313026428223\n",
      "Epoch: 1, Loss: 0.8638206124305725\n",
      "Epoch: 1, Loss: 0.8175244331359863\n",
      "Epoch: 1, Loss: 0.7040743827819824\n",
      "Epoch: 1, Loss: 0.6473942995071411\n",
      "Epoch: 1, Loss: 0.9222868084907532\n",
      "Epoch: 1, Loss: 0.6552621722221375\n",
      "Epoch: 1, Loss: 1.059050440788269\n",
      "Epoch: 1, Loss: 1.4428534507751465\n",
      "Epoch: 1, Loss: 1.052164077758789\n",
      "Epoch: 1, Loss: 1.1618939638137817\n",
      "Epoch: 1, Loss: 1.3623744249343872\n",
      "Epoch: 1, Loss: 1.4120556116104126\n",
      "Epoch: 1, Loss: 1.5541044473648071\n",
      "Epoch: 1, Loss: 1.032857894897461\n",
      "Epoch: 1, Loss: 0.8566082715988159\n",
      "Epoch: 1, Loss: 2.317868947982788\n",
      "Epoch: 1, Loss: 0.7326223254203796\n",
      "Epoch: 1, Loss: 0.9661612510681152\n",
      "Epoch: 1, Loss: 1.543238878250122\n",
      "Epoch: 1, Loss: 1.7395272254943848\n",
      "Epoch: 1, Loss: 0.9382378458976746\n",
      "Epoch: 1, Loss: 0.7428756952285767\n",
      "Epoch: 1, Loss: 0.7228739261627197\n",
      "Epoch: 1, Loss: 1.2355729341506958\n",
      "Epoch: 1, Loss: 1.3225083351135254\n",
      "Epoch: 1, Loss: 1.0089755058288574\n",
      "Epoch: 1, Loss: 0.9492326974868774\n",
      "Epoch: 1, Loss: 0.940673828125\n",
      "Epoch: 1, Loss: 1.592890977859497\n",
      "Epoch: 1, Loss: 1.010263204574585\n",
      "Epoch: 1, Loss: 0.9448671340942383\n",
      "Epoch: 1, Loss: 1.0186059474945068\n",
      "Epoch: 1, Loss: 0.5473548769950867\n",
      "Epoch: 1, Loss: 1.2709078788757324\n",
      "Epoch: 1, Loss: 0.693146824836731\n",
      "Epoch: 1, Loss: 1.47365403175354\n",
      "Epoch: 1, Loss: 1.6643667221069336\n",
      "Epoch: 1, Loss: 0.5639060735702515\n",
      "Epoch: 1, Loss: 0.645837664604187\n",
      "Epoch: 1, Loss: 1.2507333755493164\n",
      "Epoch: 1, Loss: 0.40158551931381226\n",
      "Epoch: 1, Loss: 0.7382206320762634\n",
      "Epoch: 1, Loss: 1.419698715209961\n",
      "Epoch: 1, Loss: 0.816022515296936\n",
      "Epoch: 1, Loss: 1.304715871810913\n",
      "Epoch: 1, Loss: 1.3585189580917358\n",
      "Epoch: 1, Loss: 1.1317747831344604\n",
      "Epoch: 1, Loss: 1.350827693939209\n",
      "Epoch: 1, Loss: 1.2134621143341064\n",
      "Epoch: 1, Loss: 0.771615743637085\n",
      "Epoch: 1, Loss: 1.067048192024231\n",
      "Epoch: 1, Loss: 0.617688775062561\n",
      "Epoch: 1, Loss: 0.2671022117137909\n",
      "Epoch: 1, Loss: 1.4465970993041992\n",
      "Epoch: 1, Loss: 0.9132578372955322\n",
      "Epoch: 1, Loss: 0.9247567653656006\n",
      "Epoch: 1, Loss: 0.9842961430549622\n",
      "Epoch: 1, Loss: 0.6325511932373047\n",
      "Epoch: 1, Loss: 1.5411503314971924\n",
      "Epoch: 1, Loss: 1.0188723802566528\n",
      "Epoch: 1, Loss: 1.6169315576553345\n",
      "Epoch: 1, Loss: 0.9626652598381042\n",
      "Epoch: 1, Loss: 1.192135214805603\n",
      "Epoch: 1, Loss: 0.7854089140892029\n",
      "Epoch: 1, Loss: 1.3226704597473145\n",
      "Epoch: 1, Loss: 1.0551230907440186\n",
      "Epoch: 1, Loss: 1.2488442659378052\n",
      "Epoch: 1, Loss: 1.2019872665405273\n",
      "Epoch: 1, Loss: 1.128250002861023\n",
      "Epoch: 1, Loss: 0.9777280688285828\n",
      "Epoch: 1, Loss: 0.8708357810974121\n",
      "Epoch: 1, Loss: 1.1014792919158936\n",
      "Epoch: 1, Loss: 1.1425553560256958\n",
      "Epoch: 1, Loss: 1.4578685760498047\n",
      "Epoch: 1, Loss: 0.5002133250236511\n",
      "Epoch: 1, Loss: 1.0900170803070068\n",
      "Epoch: 1, Loss: 1.6424936056137085\n",
      "Epoch: 1, Loss: 1.2196424007415771\n",
      "Epoch: 1, Loss: 0.999360978603363\n",
      "Epoch: 1, Loss: 0.6993898153305054\n",
      "Epoch: 1, Loss: 1.0306529998779297\n",
      "Epoch: 1, Loss: 1.3071224689483643\n",
      "Epoch: 1, Loss: 0.9721052646636963\n",
      "Epoch: 1, Loss: 1.00145423412323\n",
      "Epoch: 1, Loss: 0.7938359975814819\n",
      "Epoch: 1, Loss: 1.3493773937225342\n",
      "Epoch: 1, Loss: 1.2726192474365234\n",
      "Epoch: 1, Loss: 0.9753738045692444\n",
      "Epoch: 1, Loss: 1.8518612384796143\n",
      "Epoch: 1, Loss: 0.5775801539421082\n",
      "Epoch: 1, Loss: 0.8021214008331299\n",
      "Epoch: 1, Loss: 1.0066721439361572\n",
      "Epoch: 1, Loss: 1.4096181392669678\n",
      "Epoch: 1, Loss: 0.7782567739486694\n",
      "Epoch: 1, Loss: 0.8022680282592773\n",
      "Epoch: 1, Loss: 1.0340948104858398\n",
      "Epoch: 1, Loss: 1.513789415359497\n",
      "Epoch: 1, Loss: 0.9687691926956177\n",
      "Epoch: 1, Loss: 0.6269789934158325\n",
      "Epoch: 1, Loss: 1.283549427986145\n",
      "Epoch: 1, Loss: 1.1624424457550049\n",
      "Epoch: 1, Loss: 1.321832537651062\n",
      "Epoch: 1, Loss: 0.8457092046737671\n",
      "Epoch: 1, Loss: 0.833439826965332\n",
      "Epoch: 1, Loss: 1.12209951877594\n",
      "Epoch: 1, Loss: 1.3526166677474976\n",
      "Epoch: 1, Loss: 1.2032324075698853\n",
      "Epoch: 1, Loss: 1.3247320652008057\n",
      "Epoch: 1, Loss: 0.6866450905799866\n",
      "Epoch: 1, Loss: 0.8377841114997864\n",
      "Epoch: 1, Loss: 1.0261119604110718\n",
      "Epoch: 1, Loss: 1.3446623086929321\n",
      "Epoch: 1, Loss: 1.541533350944519\n",
      "Epoch: 1, Loss: 0.958905816078186\n",
      "Epoch: 1, Loss: 0.9762121438980103\n",
      "Epoch: 1, Loss: 1.0840259790420532\n",
      "Epoch: 1, Loss: 1.08646559715271\n",
      "Epoch: 1, Loss: 1.075783610343933\n",
      "Epoch: 1, Loss: 0.9579064249992371\n",
      "Epoch: 1, Loss: 0.5267817974090576\n",
      "Epoch: 1, Loss: 0.621226966381073\n",
      "Epoch: 1, Loss: 1.5123417377471924\n",
      "Epoch: 1, Loss: 0.8993532657623291\n",
      "Epoch: 1, Loss: 1.0060306787490845\n",
      "Epoch: 1, Loss: 1.0050654411315918\n",
      "Epoch: 1, Loss: 1.307881236076355\n",
      "Epoch: 1, Loss: 0.9522498846054077\n",
      "Epoch: 1, Loss: 0.8048909902572632\n",
      "Epoch: 1, Loss: 0.8497692346572876\n",
      "Epoch: 1, Loss: 0.6009044051170349\n",
      "Epoch: 1, Loss: 0.5594936609268188\n",
      "Epoch: 1, Loss: 1.5848904848098755\n",
      "Epoch: 1, Loss: 1.162082552909851\n",
      "Epoch: 1, Loss: 0.9546977877616882\n",
      "Epoch: 1, Loss: 1.2045202255249023\n",
      "Epoch: 1, Loss: 1.4247009754180908\n",
      "Epoch: 1, Loss: 0.7450277209281921\n",
      "Epoch: 1, Loss: 1.75839364528656\n",
      "Epoch: 1, Loss: 0.47487446665763855\n",
      "Epoch: 1, Loss: 0.9652410745620728\n",
      "Epoch: 1, Loss: 0.6558951735496521\n",
      "Epoch: 1, Loss: 0.7892177104949951\n",
      "Epoch: 1, Loss: 1.3561946153640747\n",
      "Epoch: 1, Loss: 0.8388047814369202\n",
      "Epoch: 1, Loss: 0.9095285534858704\n",
      "Epoch: 1, Loss: 1.3126423358917236\n",
      "Epoch: 1, Loss: 1.2377551794052124\n",
      "Epoch: 1, Loss: 0.9187901020050049\n",
      "Epoch: 1, Loss: 1.0391374826431274\n",
      "Epoch: 1, Loss: 0.9252793192863464\n",
      "Epoch: 1, Loss: 1.0804293155670166\n",
      "Epoch: 1, Loss: 0.8424202799797058\n",
      "Epoch: 1, Loss: 1.7996995449066162\n",
      "Epoch: 1, Loss: 1.0169217586517334\n",
      "Epoch: 1, Loss: 2.034691095352173\n",
      "Epoch: 1, Loss: 0.6477580666542053\n",
      "Epoch: 1, Loss: 1.1751469373703003\n",
      "Epoch: 1, Loss: 1.1716365814208984\n",
      "Epoch: 1, Loss: 0.7093396782875061\n",
      "Epoch: 1, Loss: 1.0488494634628296\n",
      "Epoch: 1, Loss: 1.0100123882293701\n",
      "Epoch: 1, Loss: 0.7823470234870911\n",
      "Epoch: 1, Loss: 0.5993459820747375\n",
      "Epoch: 1, Loss: 0.3568110764026642\n",
      "Epoch: 1, Loss: 1.030566930770874\n",
      "Epoch: 1, Loss: 1.029819130897522\n",
      "Epoch: 1, Loss: 1.4014012813568115\n",
      "Epoch: 1, Loss: 0.9393584132194519\n",
      "Epoch: 1, Loss: 1.1771008968353271\n",
      "Epoch: 1, Loss: 1.2328640222549438\n",
      "Epoch: 1, Loss: 1.416447639465332\n",
      "Epoch: 1, Loss: 0.6887072920799255\n",
      "Epoch: 1, Loss: 0.9669092297554016\n",
      "Epoch: 1, Loss: 1.1924229860305786\n",
      "Epoch: 1, Loss: 1.4637681245803833\n",
      "Epoch: 1, Loss: 0.635837197303772\n",
      "Epoch: 1, Loss: 0.9841329455375671\n",
      "Epoch: 1, Loss: 1.0771923065185547\n",
      "Epoch: 1, Loss: 1.1372132301330566\n",
      "Epoch: 1, Loss: 1.3638592958450317\n",
      "Epoch: 1, Loss: 1.3570966720581055\n",
      "Epoch: 1, Loss: 1.2023659944534302\n",
      "Epoch: 1, Loss: 0.8733981847763062\n",
      "Epoch: 1, Loss: 0.7762361764907837\n",
      "Epoch: 1, Loss: 1.3490442037582397\n",
      "Epoch: 1, Loss: 1.3505606651306152\n",
      "Epoch: 1, Loss: 1.3040562868118286\n",
      "Epoch: 1, Loss: 1.0660130977630615\n",
      "Epoch: 1, Loss: 1.4523086547851562\n",
      "Epoch: 1, Loss: 0.5685545206069946\n",
      "Epoch: 1, Loss: 1.4791591167449951\n",
      "Epoch: 1, Loss: 1.309878945350647\n",
      "Epoch: 1, Loss: 1.2286615371704102\n",
      "Epoch: 1, Loss: 0.7536200284957886\n",
      "Epoch: 1, Loss: 1.3716840744018555\n",
      "Epoch: 1, Loss: 1.113893985748291\n",
      "Epoch: 1, Loss: 1.415450930595398\n",
      "Epoch: 1, Loss: 1.032902479171753\n",
      "Epoch: 1, Loss: 0.48054808378219604\n",
      "Epoch: 1, Loss: 0.9207293391227722\n",
      "Epoch: 1, Loss: 1.2258986234664917\n",
      "Epoch: 1, Loss: 0.6424623131752014\n",
      "Epoch: 2, Loss: 1.1971538066864014\n",
      "Epoch: 2, Loss: 1.260677456855774\n",
      "Epoch: 2, Loss: 0.4450090229511261\n",
      "Epoch: 2, Loss: 1.5602052211761475\n",
      "Epoch: 2, Loss: 1.396636724472046\n",
      "Epoch: 2, Loss: 0.9856590032577515\n",
      "Epoch: 2, Loss: 0.8531908988952637\n",
      "Epoch: 2, Loss: 0.7330846190452576\n",
      "Epoch: 2, Loss: 0.5936601161956787\n",
      "Epoch: 2, Loss: 0.8833613991737366\n",
      "Epoch: 2, Loss: 0.4732012450695038\n",
      "Epoch: 2, Loss: 0.7799562215805054\n",
      "Epoch: 2, Loss: 1.1522091627120972\n",
      "Epoch: 2, Loss: 0.6032072305679321\n",
      "Epoch: 2, Loss: 1.2091022729873657\n",
      "Epoch: 2, Loss: 0.8420849442481995\n",
      "Epoch: 2, Loss: 1.067834496498108\n",
      "Epoch: 2, Loss: 0.7574051022529602\n",
      "Epoch: 2, Loss: 0.8698256015777588\n",
      "Epoch: 2, Loss: 1.2697877883911133\n",
      "Epoch: 2, Loss: 0.5836716890335083\n",
      "Epoch: 2, Loss: 0.9380801320075989\n",
      "Epoch: 2, Loss: 1.0679576396942139\n",
      "Epoch: 2, Loss: 1.2255597114562988\n",
      "Epoch: 2, Loss: 0.9192349314689636\n",
      "Epoch: 2, Loss: 0.9860556125640869\n",
      "Epoch: 2, Loss: 1.086866021156311\n",
      "Epoch: 2, Loss: 1.7059975862503052\n",
      "Epoch: 2, Loss: 1.3265140056610107\n",
      "Epoch: 2, Loss: 0.9704453349113464\n",
      "Epoch: 2, Loss: 1.071994662284851\n",
      "Epoch: 2, Loss: 0.9253435134887695\n",
      "Epoch: 2, Loss: 1.1081640720367432\n",
      "Epoch: 2, Loss: 1.556381344795227\n",
      "Epoch: 2, Loss: 1.387794852256775\n",
      "Epoch: 2, Loss: 0.9276096820831299\n",
      "Epoch: 2, Loss: 1.2978280782699585\n",
      "Epoch: 2, Loss: 0.3380547761917114\n",
      "Epoch: 2, Loss: 0.9533970952033997\n",
      "Epoch: 2, Loss: 0.36211612820625305\n",
      "Epoch: 2, Loss: 0.4271106421947479\n",
      "Epoch: 2, Loss: 0.9394906759262085\n",
      "Epoch: 2, Loss: 0.9377087354660034\n",
      "Epoch: 2, Loss: 0.7280921339988708\n",
      "Epoch: 2, Loss: 1.014719009399414\n",
      "Epoch: 2, Loss: 0.5072553157806396\n",
      "Epoch: 2, Loss: 0.7430071830749512\n",
      "Epoch: 2, Loss: 1.5396524667739868\n",
      "Epoch: 2, Loss: 0.89548259973526\n",
      "Epoch: 2, Loss: 1.2421913146972656\n",
      "Epoch: 2, Loss: 1.3567733764648438\n",
      "Epoch: 2, Loss: 0.9934819340705872\n",
      "Epoch: 2, Loss: 1.4104344844818115\n",
      "Epoch: 2, Loss: 1.1446555852890015\n",
      "Epoch: 2, Loss: 1.0584731101989746\n",
      "Epoch: 2, Loss: 0.7663477659225464\n",
      "Epoch: 2, Loss: 1.1010671854019165\n",
      "Epoch: 2, Loss: 0.9582102298736572\n",
      "Epoch: 2, Loss: 0.9005944728851318\n",
      "Epoch: 2, Loss: 0.9021151065826416\n",
      "Epoch: 2, Loss: 1.1203840970993042\n",
      "Epoch: 2, Loss: 1.3433445692062378\n",
      "Epoch: 2, Loss: 1.2889279127120972\n",
      "Epoch: 2, Loss: 1.5187091827392578\n",
      "Epoch: 2, Loss: 0.4064474105834961\n",
      "Epoch: 2, Loss: 0.9089832305908203\n",
      "Epoch: 2, Loss: 1.004730463027954\n",
      "Epoch: 2, Loss: 1.2513911724090576\n",
      "Epoch: 2, Loss: 1.3805973529815674\n",
      "Epoch: 2, Loss: 0.6858119964599609\n",
      "Epoch: 2, Loss: 1.1788729429244995\n",
      "Epoch: 2, Loss: 1.559783935546875\n",
      "Epoch: 2, Loss: 0.9358344674110413\n",
      "Epoch: 2, Loss: 1.165294885635376\n",
      "Epoch: 2, Loss: 0.7035008072853088\n",
      "Epoch: 2, Loss: 1.4595918655395508\n",
      "Epoch: 2, Loss: 1.4091228246688843\n",
      "Epoch: 2, Loss: 1.1156831979751587\n",
      "Epoch: 2, Loss: 1.196409821510315\n",
      "Epoch: 2, Loss: 0.6527935862541199\n",
      "Epoch: 2, Loss: 1.0447057485580444\n",
      "Epoch: 2, Loss: 0.9751949310302734\n",
      "Epoch: 2, Loss: 0.5893926024436951\n",
      "Epoch: 2, Loss: 1.086531400680542\n",
      "Epoch: 2, Loss: 0.9528914093971252\n",
      "Epoch: 2, Loss: 1.0552194118499756\n",
      "Epoch: 2, Loss: 0.4849962890148163\n",
      "Epoch: 2, Loss: 1.1461851596832275\n",
      "Epoch: 2, Loss: 0.5495535135269165\n",
      "Epoch: 2, Loss: 1.4748966693878174\n",
      "Epoch: 2, Loss: 0.5417845845222473\n",
      "Epoch: 2, Loss: 0.3061898946762085\n",
      "Epoch: 2, Loss: 0.8360360860824585\n",
      "Epoch: 2, Loss: 0.6246438026428223\n",
      "Epoch: 2, Loss: 0.46787530183792114\n",
      "Epoch: 2, Loss: 1.1897392272949219\n",
      "Epoch: 2, Loss: 0.815070629119873\n",
      "Epoch: 2, Loss: 1.3567253351211548\n",
      "Epoch: 2, Loss: 0.9219489693641663\n",
      "Epoch: 2, Loss: 0.9685257077217102\n",
      "Epoch: 2, Loss: 1.6372326612472534\n",
      "Epoch: 2, Loss: 0.76368647813797\n",
      "Epoch: 2, Loss: 1.004789113998413\n",
      "Epoch: 2, Loss: 0.7965084314346313\n",
      "Epoch: 2, Loss: 0.9882199764251709\n",
      "Epoch: 2, Loss: 0.7941551804542542\n",
      "Epoch: 2, Loss: 1.5609543323516846\n",
      "Epoch: 2, Loss: 1.281333088874817\n",
      "Epoch: 2, Loss: 0.7952725887298584\n",
      "Epoch: 2, Loss: 0.8593536615371704\n",
      "Epoch: 2, Loss: 0.9993664026260376\n",
      "Epoch: 2, Loss: 0.6923088431358337\n",
      "Epoch: 2, Loss: 0.44727766513824463\n",
      "Epoch: 2, Loss: 0.891914427280426\n",
      "Epoch: 2, Loss: 0.9117358326911926\n",
      "Epoch: 2, Loss: 0.9416978359222412\n",
      "Epoch: 2, Loss: 0.5785617232322693\n",
      "Epoch: 2, Loss: 1.1656893491744995\n",
      "Epoch: 2, Loss: 0.524535596370697\n",
      "Epoch: 2, Loss: 0.88261878490448\n",
      "Epoch: 2, Loss: 0.9905003905296326\n",
      "Epoch: 2, Loss: 1.0351957082748413\n",
      "Epoch: 2, Loss: 1.1182966232299805\n",
      "Epoch: 2, Loss: 0.9447646141052246\n",
      "Epoch: 2, Loss: 1.0463039875030518\n",
      "Epoch: 2, Loss: 1.253513216972351\n",
      "Epoch: 2, Loss: 0.8751508593559265\n",
      "Epoch: 2, Loss: 1.315399169921875\n",
      "Epoch: 2, Loss: 1.083837866783142\n",
      "Epoch: 2, Loss: 0.9731343984603882\n",
      "Epoch: 2, Loss: 0.798041820526123\n",
      "Epoch: 2, Loss: 0.8436596989631653\n",
      "Epoch: 2, Loss: 0.7843841910362244\n",
      "Epoch: 2, Loss: 1.1636511087417603\n",
      "Epoch: 2, Loss: 0.4304162263870239\n",
      "Epoch: 2, Loss: 1.087981104850769\n",
      "Epoch: 2, Loss: 0.9448245167732239\n",
      "Epoch: 2, Loss: 0.49448162317276\n",
      "Epoch: 2, Loss: 1.5870434045791626\n",
      "Epoch: 2, Loss: 0.7988709211349487\n",
      "Epoch: 2, Loss: 0.6483371257781982\n",
      "Epoch: 2, Loss: 1.445093035697937\n",
      "Epoch: 2, Loss: 1.127791404724121\n",
      "Epoch: 2, Loss: 1.128415584564209\n",
      "Epoch: 2, Loss: 1.3142430782318115\n",
      "Epoch: 2, Loss: 0.8243857026100159\n",
      "Epoch: 2, Loss: 0.7958066463470459\n",
      "Epoch: 2, Loss: 0.9892126321792603\n",
      "Epoch: 2, Loss: 0.9329750537872314\n",
      "Epoch: 2, Loss: 0.5895626544952393\n",
      "Epoch: 2, Loss: 0.6494330167770386\n",
      "Epoch: 2, Loss: 0.778526246547699\n",
      "Epoch: 2, Loss: 1.030436635017395\n",
      "Epoch: 2, Loss: 1.2286494970321655\n",
      "Epoch: 2, Loss: 1.3734678030014038\n",
      "Epoch: 2, Loss: 1.4097312688827515\n",
      "Epoch: 2, Loss: 1.200901985168457\n",
      "Epoch: 2, Loss: 0.6780185699462891\n",
      "Epoch: 2, Loss: 1.1892445087432861\n",
      "Epoch: 2, Loss: 0.5042781233787537\n",
      "Epoch: 2, Loss: 0.6807876825332642\n",
      "Epoch: 2, Loss: 1.5721657276153564\n",
      "Epoch: 2, Loss: 1.3107619285583496\n",
      "Epoch: 2, Loss: 0.7758482694625854\n",
      "Epoch: 2, Loss: 1.4631361961364746\n",
      "Epoch: 2, Loss: 0.9085159301757812\n",
      "Epoch: 2, Loss: 1.2546017169952393\n",
      "Epoch: 2, Loss: 0.8408356308937073\n",
      "Epoch: 2, Loss: 1.4675352573394775\n",
      "Epoch: 2, Loss: 1.0561764240264893\n",
      "Epoch: 2, Loss: 0.7846687436103821\n",
      "Epoch: 2, Loss: 1.112410545349121\n",
      "Epoch: 2, Loss: 1.1475186347961426\n",
      "Epoch: 2, Loss: 0.4259738624095917\n",
      "Epoch: 2, Loss: 1.3389437198638916\n",
      "Epoch: 2, Loss: 1.0832310914993286\n",
      "Epoch: 2, Loss: 1.203091025352478\n",
      "Epoch: 2, Loss: 0.8071944117546082\n",
      "Epoch: 2, Loss: 0.9080743193626404\n",
      "Epoch: 2, Loss: 0.7787421345710754\n",
      "Epoch: 2, Loss: 1.2641972303390503\n",
      "Epoch: 2, Loss: 0.82952880859375\n",
      "Epoch: 2, Loss: 1.1512033939361572\n",
      "Epoch: 2, Loss: 1.208177089691162\n",
      "Epoch: 2, Loss: 0.739015519618988\n",
      "Epoch: 2, Loss: 1.0055900812149048\n",
      "Epoch: 2, Loss: 1.1223386526107788\n",
      "Epoch: 2, Loss: 0.9712740778923035\n",
      "Epoch: 2, Loss: 0.7483380436897278\n",
      "Epoch: 2, Loss: 0.669916033744812\n",
      "Epoch: 2, Loss: 0.4420306086540222\n",
      "Epoch: 2, Loss: 0.36442631483078003\n",
      "Epoch: 2, Loss: 1.3367657661437988\n",
      "Epoch: 2, Loss: 1.2652987241744995\n",
      "Epoch: 2, Loss: 0.6522037982940674\n",
      "Epoch: 2, Loss: 1.2785441875457764\n",
      "Epoch: 2, Loss: 1.408138394355774\n",
      "Epoch: 2, Loss: 1.0068999528884888\n",
      "Epoch: 2, Loss: 0.9026956558227539\n",
      "Epoch: 2, Loss: 1.1333751678466797\n",
      "Epoch: 2, Loss: 1.0968672037124634\n",
      "Epoch: 2, Loss: 0.9744890332221985\n",
      "Epoch: 2, Loss: 0.985352098941803\n",
      "Epoch: 2, Loss: 0.8279378414154053\n",
      "Epoch: 2, Loss: 0.8700998425483704\n",
      "Epoch: 2, Loss: 0.8249183893203735\n",
      "Epoch: 2, Loss: 1.32648503780365\n",
      "Epoch: 2, Loss: 1.0002669095993042\n",
      "Epoch: 2, Loss: 1.3462868928909302\n",
      "Epoch: 2, Loss: 0.9852824211120605\n",
      "Epoch: 2, Loss: 0.6267887949943542\n",
      "Epoch: 2, Loss: 0.888046145439148\n",
      "Epoch: 2, Loss: 0.8576851487159729\n",
      "Epoch: 2, Loss: 0.4396299719810486\n",
      "Epoch: 2, Loss: 0.724847137928009\n",
      "Epoch: 2, Loss: 0.6559430360794067\n",
      "Epoch: 2, Loss: 0.8071182370185852\n",
      "Epoch: 2, Loss: 1.02978515625\n",
      "Epoch: 2, Loss: 0.8412843346595764\n",
      "Epoch: 2, Loss: 1.1123210191726685\n",
      "Epoch: 2, Loss: 1.0867655277252197\n",
      "Epoch: 2, Loss: 0.6302193403244019\n",
      "Epoch: 2, Loss: 0.4344193637371063\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(3):  # Maintain or adjust the number of epochs as necessary\n",
    "    for batch in train_loader:\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IADbtsFRBxT",
    "outputId": "e00d9b5e-d312-4ed6-cfcc-631df57539f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./saved_bart_model/tokenizer_config.json',\n",
       " './saved_bart_model/special_tokens_map.json',\n",
       " './saved_bart_model/vocab.json',\n",
       " './saved_bart_model/merges.txt',\n",
       " './saved_bart_model/added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def compute_rouge(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        score = scorer.score(ref, pred)\n",
    "        scores['rouge1'].append(score['rouge1'].fmeasure)\n",
    "        scores['rouge2'].append(score['rouge2'].fmeasure)\n",
    "        scores['rougeL'].append(score['rougeL'].fmeasure)\n",
    "\n",
    "    # Average the scores\n",
    "    avg_scores = {key: sum(values) / len(values) for key, values in scores.items()}\n",
    "    return avg_scores\n",
    "\n",
    "# Example usage within an evaluation function\n",
    "def evaluate_model(model, data_loader, tokenizer, device='cuda'):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=150, num_beams=5)\n",
    "            decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            all_preds.extend(decoded_preds)\n",
    "            all_labels.extend(decoded_labels)\n",
    "\n",
    "    # Compute ROUGE scores using the custom function\n",
    "    rouge_scores = compute_rouge(all_preds, all_labels)\n",
    "    return rouge_scores\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./saved_bart_model')\n",
    "tokenizer.save_pretrained('./saved_bart_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6xhAYAN8SI5D"
   },
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Path to the saved model\n",
    "model_dir = './saved_bart_model'\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BartTokenizer.from_pretrained(model_dir)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_dIjCRAoTVp7"
   },
   "outputs": [],
   "source": [
    "def prepare_input(text, tokenizer, max_length=512):\n",
    "    inputs = tokenizer.encode_plus(text, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length')\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "dzpG5NyOTYFH"
   },
   "outputs": [],
   "source": [
    "def generate_summary(input_text, tokenizer, model, device='cpu'):\n",
    "    # Prepare the model and input data\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    inputs = prepare_input(input_text, tokenizer)\n",
    "\n",
    "    # Generate summary\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=150, num_beams=5, early_stopping=True)\n",
    "\n",
    "    # Decode generated ids to text\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sWduA4PTbem",
    "outputId": "74f34e45-0d2f-42f4-efc5-ae6883431fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: This famous sentence contains every letter in the English language, making it a pangram used in typing practice and testing typewriters.\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "input_text = \"The quick brown fox jumps over the lazy dog. This famous sentence contains every letter in the English language, making it a pangram used in typing practice and testing typewriters.\"\n",
    "\n",
    "# Generate the summary using the updated function\n",
    "summary = generate_summary(input_text, tokenizer, model)\n",
    "print(\"Generated Summary:\", summary)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
